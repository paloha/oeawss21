{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c80f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import PIL, urllib\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "document.title = \"My OEAW SummerSchool App\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_url = 'https://upload.wikimedia.org/wikipedia/commons/2/24/Blue_Tshirt.jpg'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(widgets.HTML('''\n",
    "<h1>\n",
    "My OEAW Summer School App\n",
    "</h1>\n",
    "<p style=\"max-width: 500px;\">\n",
    "This app takes a url to a .jpg or .png image.\n",
    "Converts it into a grey scale 28x28 image and passes\n",
    "it through neural network trained on FashionMNIST dataset.\n",
    "The predicted class is than simply displayed as a text.\n",
    "Type in an image url and press enter.\n",
    "</p>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # input layer\n",
    "        self.linear1 = nn.Linear(784, 1024)\n",
    "        #hidden layers\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.linear3 = nn.Linear(2048, 1024)\n",
    "        self.linear4 = nn.Linear(1024, 512)\n",
    "        #output layer\n",
    "        self.linear5 = nn.Linear(512, 10)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.2)  # values changed in exercise 3\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "        \n",
    "        x = self.linear4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.selu(x)\n",
    "        \n",
    "        x = self.linear5(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    # CNN architecture (two conv layers followed by three fully connected layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dropout2 = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d457de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(mt=widgets.RadioButtons(\n",
    "    options=['CNN', 'NN'],\n",
    "    value='CNN',\n",
    "    description='Model:',\n",
    "    disabled=False\n",
    "))\n",
    "def f(mt):\n",
    "    if mt == 'CNN':\n",
    "        model = ConvNet()\n",
    "        model_path = 'trained_cnn.pt'\n",
    "        shape = (28,28)\n",
    "    else:\n",
    "        model = NeuralNet()\n",
    "        model_path = 'trained_model.pt'\n",
    "        shape=(784,)\n",
    "\n",
    "    # Load the desired model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device));\n",
    "    model.eval()\n",
    "    \n",
    "    @interact(\n",
    "        url=widgets.Text(\n",
    "            value=default_url,\n",
    "            placeholder='Type in an image URL.',\n",
    "            description='Image URL:',\n",
    "            disabled=False,\n",
    "            continuous_update=False))\n",
    "    def g(url):\n",
    "        try:\n",
    "            # Load and transform the image\n",
    "            img_arr = PIL.Image.open(\n",
    "                urllib.request.urlopen(url)).convert('L').resize((28,28))\n",
    "\n",
    "            # Plot the transformed image\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img_arr, cmap='gray', aspect='equal')\n",
    "            plt.title(f'Converted into grayscale {img_arr._size} image')\n",
    "            plt.axis('off')\n",
    "            plt.show()   \n",
    "\n",
    "            # Convert the image to tensor\n",
    "            data = torch.tensor(np.array(img_arr), dtype=torch.float32).view(1, 1, *shape)\n",
    "            data = transforms.Normalize((0.1307,), (0.3081,))(data)\n",
    "            \n",
    "            # Pass it through the model and display the result\n",
    "            with torch.no_grad():\n",
    "                display(FashionMNIST.classes[model(data).argmax().item()])\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
